{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmCizOX81M3mt/Cwazq1ze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanupratap/Approximate-Divider/blob/main/SP500_Prediction_(CNNLSTM_SARIMA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_TCmuD6iouZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sp500_data(start='2010-01-01'):\n",
        "    \"\"\"Download S&P500 data\"\"\"\n",
        "    df = yf.download('^GSPC', start=start)\n",
        "    return df"
      ],
      "metadata": {
        "id": "GAX84GdtmrPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARIMA Model Prediction"
      ],
      "metadata": {
        "id": "ppeqFa_BmyX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = np.sqrt(mean_squared_error(test_data[:100], predictions))"
      ],
      "metadata": {
        "id": "sqzFGQlssp6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "TESTING_UNTIL = 100 # limiting iterations\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_size = int(len(data) * 0.8)\n",
        "train_data = data[:train_size]\n",
        "test_data = data[train_size:]\n",
        "\n",
        "# Fit the model on training data\n",
        "model = SARIMAX(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 0, 12))\n",
        "sarima_result = model.fit(disp=False)\n",
        "\n",
        "# Predict one step ahead iteratively for the test set\n",
        "\n",
        "predictions = []\n",
        "for i in range(min(TESTING_UNTIL,len(test_data))):\n",
        "    if i % 10 == 0:  # Log progress every 10 iterations\n",
        "          print(f\"Iteration {i}/{len(test_data)}: Predicting for {test_data.index[i]}\")\n",
        "    model_forecast = sarima_result.get_forecast(steps=1)\n",
        "    pred = model_forecast.predicted_mean.iloc[0]\n",
        "    predictions.append(pred)\n",
        "    sarima_result = sarima_result.append(test_data.iloc[i:i+1])\n",
        "\n",
        "# Calculate performance metrics\n",
        "rmse = np.sqrt(mean_squared_error(test_data[:TESTING_UNTIL], predictions))\n",
        "mape = mean_absolute_percentage_error(test_data, predictions)\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2%}\")\n",
        "\n",
        "# Plot actual vs predicted\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_data.index, test_data, label=\"Actual\", color=\"blue\")\n",
        "plt.plot(test_data.index, predictions, label=\"Predicted\", color=\"orange\")\n",
        "plt.title(\"S&P 500 t+1 Predictions\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4YYkLnc8neQM",
        "outputId": "720a6938-b832-49f1-ba81-168e4be6216b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0/783: Predicting for 2021-12-31 00:00:00\n",
            "Iteration 10/783: Predicting for 2022-01-14 00:00:00\n",
            "Iteration 20/783: Predicting for 2022-01-28 00:00:00\n",
            "Iteration 30/783: Predicting for 2022-02-11 00:00:00\n",
            "Iteration 40/783: Predicting for 2022-02-25 00:00:00\n",
            "Iteration 50/783: Predicting for 2022-03-11 00:00:00\n",
            "Iteration 60/783: Predicting for 2022-03-25 00:00:00\n",
            "Iteration 70/783: Predicting for 2022-04-08 00:00:00\n",
            "Iteration 80/783: Predicting for 2022-04-22 00:00:00\n",
            "Iteration 90/783: Predicting for 2022-05-06 00:00:00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [783, 100]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-326-6399534e4539>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Calculate performance metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[0;32m--> 565\u001b[0;31m         _check_reg_targets_with_floating_dtype(\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mdtype_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_matching_floating_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [783, 100]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN LSTM Model Prediction"
      ],
      "metadata": {
        "id": "R-zg8Ewrms7v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5juHielr-C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "  \"\"\"Get the appropriate device for M1 Mac, CUDA, or CPU\"\"\"\n",
        "  if torch.backends.mps.is_available():\n",
        "      return torch.device(\"mps\")\n",
        "  elif torch.cuda.is_available():\n",
        "      return torch.device(\"cuda\")\n",
        "  else:\n",
        "      return torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "bIyOZMqwip8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(window_size=10):\n",
        "  # Download data\n",
        "  df = yf.download('^GSPC', start='2010-01-01')\n",
        "\n",
        "  # Calculate features\n",
        "  df['Returns'] = df['Close'].pct_change()\n",
        "  df['High_Low_Ratio'] = (df['High'] / df['Low']) - 1\n",
        "  df['Open_Close_Ratio'] = (df['Close'] / df['Open']) - 1\n",
        "  df['Volume_Change'] = df['Volume'].pct_change()\n",
        "  df['MA5_Ratio'] = (df['Close'] / df['Close'].rolling(window=5).mean()) - 1\n",
        "  df['MA20_Ratio'] = (df['Close'] / df['Close'].rolling(window=20).mean()) - 1\n",
        "  df['Volatility'] = df['Returns'].rolling(window=20).std()\n",
        "\n",
        "  # Select features\n",
        "  features = ['Returns', 'High_Low_Ratio', 'Open_Close_Ratio',\n",
        "              'Volume_Change', 'MA5_Ratio', 'MA20_Ratio', 'Volatility']\n",
        "\n",
        "  # Drop NaN values\n",
        "  df = df.dropna()\n",
        "\n",
        "  # Prepare target (next day's return)\n",
        "  df['Target'] = df['Returns'].shift(-1)\n",
        "  df = df.dropna()\n",
        "\n",
        "  # Scale features\n",
        "  scaler = MinMaxScaler()\n",
        "  data = scaler.fit_transform(df[features + ['Target']])\n",
        "\n",
        "  # Create sequences\n",
        "  X, y = [], []\n",
        "  for i in range(len(data) - window_size):\n",
        "      X.append(data[i:(i + window_size), :-1])  # All features except Target\n",
        "      y.append(data[i + window_size, -1])      # Target\n",
        "\n",
        "  # Convert to arrays\n",
        "  X, y = np.array(X), np.array(y)\n",
        "\n",
        "  # Train-test split\n",
        "  train_size = int(len(X) * 0.8)\n",
        "  X_train = X[:train_size]\n",
        "  y_train = y[:train_size]\n",
        "  X_test = X[train_size:]\n",
        "  y_test = y[train_size:]\n",
        "\n",
        "  return X_train, y_train, X_test, y_test, scaler, df['Close']\n"
      ],
      "metadata": {
        "id": "JhghN9JhEV4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StockDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "      self.X = torch.FloatTensor(X)\n",
        "      self.y = torch.FloatTensor(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "YBsbrOiBitz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNNLSTM(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "      super(SimpleCNNLSTM, self).__init__()\n",
        "\n",
        "      # One CNN layer\n",
        "      self.conv = nn.Conv1d(input_dim, 32, kernel_size=3, padding=1)\n",
        "\n",
        "      # One LSTM layer\n",
        "      self.lstm = nn.LSTM(32, 32, batch_first=True)\n",
        "\n",
        "      # One dense layer\n",
        "      self.fc = nn.Linear(32, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      # CNN\n",
        "      x = x.transpose(1, 2)\n",
        "      x = torch.relu(self.conv(x))\n",
        "      x = x.transpose(1, 2)\n",
        "\n",
        "      # LSTM\n",
        "      x, _ = self.lstm(x)\n",
        "      x = x[:, -1, :]  # Take last output\n",
        "\n",
        "      # Dense\n",
        "      x = self.fc(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "WrMas53ziyDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, epochs=50):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')\n"
      ],
      "metadata": {
        "id": "E73Pdkx6DCFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(actuals, predictions, dates=None, theme='seaborn-v0_8-paper'):\n",
        "    \"\"\"Create comprehensive visualization of results\"\"\"\n",
        "    plt.style.use(theme)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # 1. Time Series Plot\n",
        "    ax1 = plt.subplot(3, 2, 1)\n",
        "    ax1.plot(actuals, label='Actual', color='blue', alpha=0.7)\n",
        "    ax1.plot(predictions, label='Predicted', color='red', alpha=0.7)\n",
        "    ax1.set_title('S&P500 Price - Actual vs Predicted')\n",
        "    ax1.set_xlabel('Time')\n",
        "    ax1.set_ylabel('Price')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # 2. Scatter Plot with Regression Line\n",
        "    ax2 = plt.subplot(3, 2, 2)\n",
        "    sns.regplot(x=actuals, y=predictions, scatter_kws={'alpha':0.5},\n",
        "                line_kws={'color': 'red'}, ax=ax2)\n",
        "    ax2.set_title('Actual vs Predicted - Regression Plot')\n",
        "    ax2.set_xlabel('Actual Price')\n",
        "    ax2.set_ylabel('Predicted Price')\n",
        "\n",
        "    # 3. Prediction Error Distribution\n",
        "    ax3 = plt.subplot(3, 2, 3)\n",
        "    errors = predictions - actuals\n",
        "    sns.histplot(errors, kde=True, ax=ax3)\n",
        "    ax3.set_title('Prediction Error Distribution')\n",
        "    ax3.set_xlabel('Prediction Error')\n",
        "    ax3.set_ylabel('Frequency')\n",
        "\n",
        "    # 4. Prediction Error vs Actual Price\n",
        "    ax4 = plt.subplot(3, 2, 4)\n",
        "    ax4.scatter(actuals, errors, alpha=0.5)\n",
        "    ax4.axhline(y=0, color='r', linestyle='--')\n",
        "    ax4.set_title('Prediction Error vs Actual Price')\n",
        "    ax4.set_xlabel('Actual Price')\n",
        "    ax4.set_ylabel('Prediction Error')\n",
        "\n",
        "    # 5. Direction Prediction Accuracy\n",
        "    ax5 = plt.subplot(3, 2, 5)\n",
        "    direction_true = np.diff(actuals) > 0\n",
        "    direction_pred = np.diff(predictions) > 0\n",
        "    direction_accuracy = (direction_true == direction_pred)\n",
        "    ax5.plot(direction_accuracy, label='Correct Direction', color='green', alpha=0.7)\n",
        "    ax5.set_title('Direction Prediction Accuracy')\n",
        "    ax5.set_xlabel('Time')\n",
        "    ax5.set_ylabel('Direction Accuracy')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True)\n",
        "\n",
        "    # 6. Rolling Direction Accuracy\n",
        "    ax6 = plt.subplot(3, 2, 6)\n",
        "    rolling_accuracy = pd.Series(direction_accuracy).rolling(window=20).mean()\n",
        "    ax6.plot(rolling_accuracy, label='20-day Rolling Accuracy', color='purple', alpha=0.7)\n",
        "    ax6.axhline(y=0.5, color='r', linestyle='--', label='Random Guess')\n",
        "    ax6.set_title('20-day Rolling Direction Accuracy')\n",
        "    ax6.set_xlabel('Time')\n",
        "    ax6.set_ylabel('Accuracy')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Xvgdq-ASDHxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Work in Progress\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.6, beta=0.4):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.alpha = alpha  # Weight for MSE\n",
        "        self.beta = beta    # Weight for direction\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # MSE Loss\n",
        "        mse_loss = self.mse(y_pred, y_true)\n",
        "\n",
        "        # Direction Loss\n",
        "        pred_diff = y_pred[1:] - y_pred[:-1]\n",
        "        true_diff = y_true[1:] - y_true[:-1]\n",
        "\n",
        "        direction_match = torch.sign(pred_diff) * torch.sign(true_diff)\n",
        "        direction_loss = -torch.mean(direction_match)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.alpha * mse_loss + self.beta * direction_loss\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "wCVc6yDzYENV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now Training the model"
      ],
      "metadata": {
        "id": "8cpDxd8RkDgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 10\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# Prepare data\n",
        "print(\"Loading and preparing data...\")\n",
        "X_train, y_train, X_test, y_test, scaler, close_prices = prepare_data(WINDOW_SIZE)\n",
        "\n",
        "# Create data loader\n",
        "train_dataset = StockDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Create and train model\n",
        "model = SimpleCNNLSTM(input_dim=X_train.shape[2])\n",
        "print(\"Training model...\")\n",
        "train_model(model, train_loader, EPOCHS)\n",
        "\n",
        "# Make predictions\n",
        "model.eval()\n",
        "test_dataset = StockDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        outputs = model(batch_X)\n",
        "        predictions.extend(outputs.squeeze().numpy())\n",
        "        actuals.extend(batch_y.numpy())\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "actuals = np.array(actuals)\n",
        "\n",
        "# Convert returns to prices\n",
        "test_close = close_prices[-len(predictions)-1:-1].values\n",
        "pred_prices = test_close * (1 + predictions)\n",
        "actual_prices = test_close * (1 + actuals)\n",
        "\n",
        "# Calculate errors\n",
        "mse = np.mean((pred_prices - actual_prices) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = np.mean(np.abs(pred_prices - actual_prices))\n",
        "mape = np.mean(np.abs((pred_prices - actual_prices) / actual_prices)) * 100\n",
        "\n",
        "print(\"\\nError Metrics:\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO9JCKlKEwL7",
        "outputId": "7e058668-13e8-4a63-c5bc-b8ead8d4e5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch [10/50], Loss: 0.0026\n",
            "Epoch [20/50], Loss: 0.0025\n",
            "Epoch [30/50], Loss: 0.0025\n",
            "Epoch [40/50], Loss: 0.0024\n",
            "Epoch [50/50], Loss: 0.0024\n",
            "\n",
            "Error Metrics:\n",
            "MSE: 61049.25\n",
            "RMSE: 247.08\n",
            "MAE: 183.40\n",
            "MAPE: 2.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ftqWNEQl17e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}